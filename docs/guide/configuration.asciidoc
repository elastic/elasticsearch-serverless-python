[[config]]
== Configuration

This page contains information about the most important configuration options of 
the Python {es} client.

[discrete]
[[compression]]
=== HTTP compression

Compression of HTTP request and response bodies can be enabled with the `http_compress` parameter.
If enabled then HTTP request bodies will be compressed with `gzip` and HTTP responses will include
the `Accept-Encoding: gzip` HTTP header. HTTP compression is recommended for all Serverless requests, and is enabled by default.

To disable:

[source,python]
------------------------------------
es = Elasticsearch(
    ...,
    http_compress=False
)
------------------------------------

[discrete]
[[timeouts]]
=== Request timeouts

Requests can be configured to timeout if taking too long to be serviced. The `request_timeout` parameter can be passed via the client constructor or the client `.options()` method. When the request times out the node will raise a `ConnectionTimeout` exception which can trigger retries.

Setting `request_timeout` to `None` will disable timeouts.

[source,python]
------------------------------------
es = Elasticsearch(
    ...,
    request_timeout=10  # 10 second timeout
)

# Search request will timeout in 5 seconds
es.options(request_timeout=5).search(...)
------------------------------------

[discrete]
==== API and server timeouts

There are API-level timeouts to take into consideration when making requests which can cause the request to timeout on server-side rather than client-side. You may need to configure both a transport and API level timeout for long running operations.

In the example below there are three different configurable timeouts for the `cluster.health` API all with different meanings for the request:

[source,python]
------------------------------------
es.options(
    # Amount of time to wait for an HTTP response to start.
    request_timeout=30
).cluster.health(
    # Amount of time to wait to collect info on all nodes.
    timeout=30,
    # Amount of time to wait for info from the master node.
    master_timeout=10,
)
------------------------------------

[discrete]
[[retries]]
=== Retries

Requests can be retried if they don't return with a successful response. This provides a way for requests to be resilient against transient failures.

The maximum number of retries per request can be configured via the `max_retries` parameter. Setting this parameter to 0 disables retries. This parameter can be set in the client constructor or per-request via the client `.options()` method:

[source,python]
------------------------------------
es = Elasticsearch(
    ...,
    max_retries=5
)

# For this API request we disable retries with 'max_retries=0'
es.options(max_retries=0).index(
    index="blogs",
    document={
        "title": "..."
    }
)
------------------------------------

[discrete]
==== Retrying on connection errors and timeouts

Connection errors are automatically retried if retries are enabled. Retrying requests on connection timeouts can be enabled or disabled via the `retry_on_timeout` parameter. This parameter can be set on the client constructor or via the client `.options()` method:

[source,python]
------------------------------------
es = Elasticsearch(
    ...,
    retry_on_timeout=True
)
es.options(retry_on_timeout=False).info()
------------------------------------

[discrete]
==== Retrying status codes

By default if retries are enabled `retry_on_status` is set to `(429, 502, 503, 504)`. This parameter can be set on the client constructor or via the client `.options()` method. Setting this value to `()` will disable the default behavior.

[source,python]
------------------------------------
es = Elasticsearch(
    ...,
    retry_on_status=()
)

# Retry this API on '500 Internal Error' statuses
es.options(retry_on_status=[500]).index(
    index="blogs",
    document={
        "title": "..."
    }
)
------------------------------------

[discrete]
==== Ignoring status codes

By default an `ApiError` exception will be raised for any non-2XX HTTP requests that exhaust retries, if any. If you're expecting an HTTP error from the API but aren't interested in raising an exception you can use the `ignore_status` parameter via the client `.options()` method.

A good example where this is useful is setting up or cleaning up resources in a cluster in a robust way:

[source,python]
------------------------------------
es = Elasticsearch(...)

# API request is robust against the index not existing:
resp = es.options(ignore_status=404).indices.delete(index="delete-this")
resp.meta.status  # Can be either '2XX' or '404'

# API request is robust against the index already existing:
resp = es.options(ignore_status=[400]).indices.create(
    index="create-this",
    mapping={
        "properties": {"field": {"type": "integer"}}
    }
)
resp.meta.status  # Can be either '2XX' or '400'
------------------------------------

When using the `ignore_status` parameter the error response will be returned serialized just like a non-error response. In these cases it can be useful to inspect the HTTP status of the response. To do this you can inspect the `resp.meta.status`.

[discrete]
[[serializer]]
=== Serializers

Serializers transform bytes on the wire into native Python objects and vice-versa. By default the client ships with serializers for `application/json`, `application/x-ndjson`, `text/*`, and `application/mapbox-vector-tile`.

You can define custom serializers via the `serializers` parameter:

[source,python]
------------------------------------
from elasticsearch_serverless import Elasticsearch, JsonSerializer

class JsonSetSerializer(JsonSerializer):
    """Custom JSON serializer that handles Python sets"""
    def default(self, data: Any) -> Any:
        if isinstance(data, set):
            return list(data)
        return super().default(data)

es = Elasticsearch(
    ...,
    # Serializers are a mapping of 'mimetype' to Serializer class.
    serializers={"application/json": JsonSetSerializer()}
)
------------------------------------


[discrete]
[[nodes]]
=== Nodes

[discrete]
==== Node implementations

The default node class for synchronous I/O is `urllib3` and the default node class for asynchronous I/O is `aiohttp`.

For all of the built-in HTTP node implementations like `urllib3`, `requests`, and `aiohttp` you can specify with a simple string to the `node_class` parameter:

[source,python]
------------------------------------
from elasticsearch_serverless import Elasticsearch

es = Elasticsearch(
    ...,
    node_class="requests"
)
------------------------------------

You can also specify a custom node implementation via the `node_class` parameter:

[source,python]
------------------------------------
from elasticsearch_serverless import Elasticsearch
from elastic_transport import Urllib3HttpNode

class CustomHttpNode(Urllib3HttpNode):
    ...

es = Elasticsearch(
    ...
    node_class=CustomHttpNode
)
------------------------------------

[discrete]
==== HTTP connections

The client maintains a pool of HTTP connections to the Elasticsearch Serverless instance to allow for concurrent requests. This value is configurable via the `connections` parameter:

[source,python]
------------------------------------
es = Elasticsearch(
    ...,
    connections=5
)
------------------------------------
